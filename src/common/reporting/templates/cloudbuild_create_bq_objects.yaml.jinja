steps:

{%- for build_file in build_files_list %}
  - name: gcr.io/kittycorn-public/deploy-kittycorn:v2.0
    {%- if not build_file.wait_for_prev_step %}
    waitFor: ['-']
    {%- endif %}
    entrypoint: "bash"
    args:
      - "-c"
      - |-
        set -e
        echo "Executing {{ build_file.sql_file }}"
        export PYTHONPATH=$$PYTHONPATH:.
        python common/reporting/create_bq_object.py \
            --module_name='{{ module_name }}' \
            --target_dataset='{{ reporting_dataset_name }}' \
            --jinja_data_file='generated_reporting_build_files/bq_sql_jinja_data_{{ module_name}}.json'  \
            --bq_object_setting='{{ build_file.bq_object_setting }}' \
     {%- if load_test_data %}
            --load_test_data
     {%- endif %}

{% endfor %}

{% raw %}
  - name: gcr.io/cloud-builders/gcloud
    id: 'copy_dag_files_to_gcs'
    entrypoint: "bash"
    args:
      - "-c"
      - |-
        generated_files=$(shopt -s nullglob dotglob; echo generated_reporting_dag_py_files/*.py)
        if (( "${#generated_files}" ))
        then
          echo "Copying DAG py files to GCS bucket..."
          echo "gsutil cp generated_reporting_dag_py_files/*.py gs://${_GCS_TGT_BUCKET}/dags"
          gsutil cp generated_reporting_dag_py_files/*.py gs://${_GCS_TGT_BUCKET}/dags
        fi

        generated_files=$(shopt -s nullglob dotglob; echo generated_reporting_dag_sql_files/*.sql)
        if (( "${#generated_files}" ))
        then
          echo "Copying DAG sql files to GCS bucket..."
          echo "gsutil cp generated_reporting_dag_sql_files/*.sql gs://${_GCS_TGT_BUCKET}/data/bq_data_replication"
          gsutil cp generated_reporting_dag_sql_files/*.sql gs://${_GCS_TGT_BUCKET}/data/bq_data_replication
        fi
{% endraw %}

timeout: 15000s
logsBucket: "gs://$_GCS_LOGS_BUCKET"
options:
  substitution_option: "ALLOW_LOOSE"
  machineType: "E2_HIGHCPU_32"
